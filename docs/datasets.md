# Datasets

## LLaVA-Instruct-150K

This project uses the LLaVA-Instruct-150K dataset, a large-scale multimodal instruction tuning dataset designed to align vision encoders with large language models.

The dataset consists of image–instruction–response triplets derived from publicly available visual datasets such as MS COCO and Visual Genome, combined with high-quality multimodal instructions generated using large language models.

LLaVA-Instruct-150K supports multiple multimodal tasks including:
- Image Captioning
- Visual Question Answering (VQA)
- Open-ended Multimodal Instruction Following

The dataset is open and permissive, making it suitable for reproducible research and alignment with open-weight language models such as GPT-OSS.
